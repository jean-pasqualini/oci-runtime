Bonne question ğŸ‘Œ
Docker nâ€™a pas disparu avec containerd, mais son rÃ´le a changÃ© : aujourdâ€™hui il dÃ©lÃ¨gue Ã©normÃ©ment, mais pas tout.

â¸»

ğŸ”¹ Architecture actuelle (Docker â€œmoderneâ€)

Quand tu lances docker run ... :
1.	Docker CLI
â€¢	Câ€™est juste le binaire docker que tu tapes dans ton shell.
â€¢	Il envoie une requÃªte HTTP Ã  dockerd (le dÃ©mon Docker).
2.	dockerd (Docker Engine)
â€¢	Câ€™est lui le vrai orchestrateur â€œDockerâ€.
â€¢	Il expose lâ€™API REST de Docker.
â€¢	Depuis Docker 1.11 (2016), dockerd ne gÃ¨re plus directement les conteneurs.
â€¢	Il dÃ©lÃ¨gue Ã  containerd la gestion des images, snapshots, et conteneurs.
3.	containerd (intÃ©grÃ© dans Docker)
â€¢	ReÃ§oit les ordres de dockerd.
â€¢	PrÃ©pare le bundle OCI (rootfs + config.json).
â€¢	GÃ¨re les snapshots (via overlayfs, btrfs, zfsâ€¦).
â€¢	Lance un shim v2 adaptÃ© au runtime choisi.
4.	shim v2 (ex: containerd-shim-runc-v2)
â€¢	Traduit lâ€™API Task de containerd en appels concrets au runtime.
5.	OCI Runtime (par dÃ©faut runc)
â€¢	Lit le config.json
â€¢	Configure namespaces, cgroups, mounts, etc.
â€¢	Lance le process du conteneur.

â¸»

ğŸ”¹ Donc, Docker fait quoi exactement ?
â€¢	Docker CLI : ergonomie (UX, options conviviales).
â€¢	dockerd :
â€¢	API REST + authentification,
â€¢	gestion des volumes, rÃ©seaux, plugins,
â€¢	intÃ©gration Swarm, secrets, etc.,
â€¢	dÃ©lÃ©gation Ã  containerd pour tout ce qui est bas-niveau.
â€¢	containerd :
â€¢	moteur de conteneurs conforme OCI,
â€¢	gÃ¨re images, snapshots, bundle, shims.
â€¢	runc :
â€¢	runtime OCI qui applique le config.json.

â¸»

ğŸ‘‰ RÃ©sumÃ© :
â€¢	Avant 2016 : Docker gÃ©rait directement tout (image â†’ conteneur via libcontainer â†’ runc).
â€¢	Depuis : Docker sâ€™appuie sur containerd pour lâ€™exÃ©cution et la gestion bas-niveau.
â€¢	Docker reste surtout une surcouche orientÃ©e dÃ©veloppeurs (CLI, API REST, UX), alors que containerd est le runtime â€œpurâ€ utilisÃ© aussi par Kubernetes.

â¸»

Veux-tu que je te fasse un schÃ©ma complet du pipeline Docker â†’ containerd â†’ shim â†’ runc pour visualiser qui fait quoi ?

------

Tu as raison : ce nâ€™est pas une bonne pratique de â€œmettre tout le runtimeâ€ dans le shim.
Le shim doit rester mince : un adaptateur qui implÃ©mente lâ€™API Task (v2) de containerd et la traduit vers ton runtime.

Pourquoi Ã©viter un â€œgros shimâ€
â€¢	SÃ©paration des responsabilitÃ©s :
â€¢	containerd : images/snapshots/bundles + orchestration.
â€¢	shim : RPC Task (Create/Start/Exec/Kill/Wait/Delete), gestion du stdio, propagation dâ€™Ã©vÃ©nements, reaper.
â€¢	runtime : application du config.json (namespaces, cgroups, mountsâ€¦), exÃ©cution.
â€¢	StabilitÃ© & maintenance : un shim gras devient un mini-containerd + mini-runtime Ã  maintenir (cgroups v1/v2, seccomp, apparmor, idmaps, mountsâ€¦).
â€¢	Isolation des pannes : un shim simple est plus robuste et remplaÃ§able sans impacter le reste.
â€¢	Ã‰volutivitÃ© : si ton runtime Ã©volue, un shim fin limite la surface Ã  mettre Ã  jour.

ModÃ¨les qui marchent bien
1.	Fork/exec dâ€™un runtime OCI (runc, crun)
â€¢	Le shim appelle un binaire runtime existant (â€¦ create/start/â€¦) et relaie lâ€™Ã©tat.
â€¢	Le plus simple et le plus courant.
2.	Daemon/runtime â€œsandboxâ€ sÃ©parÃ© (Kata, gVisor)
â€¢	Le shim parle Ã  TON dÃ©mon/agent (VM, user-space kernel, etc.) et reste un pont TTRPC.
â€¢	Le â€œgrosâ€ de la logique vit dans le runtime/agent, pas dans le shim.
3.	Librairie runtime embarquÃ©e (linker une lib type libcontainer)
â€¢	Possible, mais attention : tu rÃ©-implÃ©mentes alors toute la logique OCI dans le shim â†’ Ã  Ã©viter sauf raison impÃ©rieuse.

Ce que le shim devrait faire
â€¢	ImplÃ©menter lâ€™API Task v2 (TTRPC) : Create, Start, Kill, Exec, Wait, Delete, State, Events, Checkpoint/Restore (si support).
â€¢	GÃ©rer le lifecycle des processus, le reaping, le stdio, les PIDs/exit codes.
â€¢	Traduire les appels vers ton runtime (CLI, socket, gRPC interne, agent VM, etc.).
â€¢	Ne pas : crÃ©er config.json, gÃ©rer les images/snapshots, ni rÃ©Ã©crire la spec OCI.

Quand â€œgrossirâ€ un peu le shim ?
â€¢	Prototypage rapide.
â€¢	Besoin minimal de colle spÃ©cifique (ex : bootstrap dâ€™un agent/sandbox).
MÃªme lÃ , vise une fine couche et dÃ©place le cÅ“ur mÃ©tier dans un binaire/runtime dÃ©diÃ©.

Reco pratique (structure de projet)
â€¢	shim/ : serveur TTRPC + mapping des RPC Task â†’ appels runtime (mince).
â€¢	runtime/ : vrai moteur (daemon/CLI/lib) qui applique la spec (lourd).
â€¢	Tests dâ€™intÃ©gration via ctr/nerdctl avec un runtime_type = "io.containerd.tonruntime.v2".

En bref : garde le shim petit, mets lâ€™intelligence dans le runtime. Câ€™est plus sÃ»r, plus maintenable, et Ã§a colle Ã  lâ€™architecture containerd.

----

Yes ğŸ‘, le shim sâ€™appuie sur le layout dâ€™Ã©tat que maintient le runtime (ex. runc) sous le --root quâ€™on lui passe.

â¸»

ğŸ”¹ ConcrÃ¨tement avec containerd-shim-runc-v2

Quand containerd demande au shim :

runc --root /run/containerd/runc --bundle ... create <id>

	â€¢	runc crÃ©e un dossier dâ€™Ã©tat :

/run/containerd/runc/<id>/
â”œâ”€ state.json      # Ã©tat courant du conteneur
â”œâ”€ pid            # PID du process init
â”œâ”€ attach         # fifos pour stdio
â””â”€ ...


	â€¢	Ce layout contient tout ce que le runtime doit connaÃ®tre pour manipuler le conteneur aprÃ¨s le create.
	â€¢	Quand le shim reÃ§oit un State RPC de containerd, il appelle en rÃ©alitÃ© :

runc --root /run/containerd/runc state <id>

et parse le JSON renvoyÃ© (qui est basÃ© sur les fichiers dans ce rÃ©pertoire dâ€™Ã©tat).

	â€¢	Pour un Kill, Exec, Delete, etc., le shim passe aussi par ce rÃ©pertoire (--root) pour retrouver le conteneur et agir dessus.

â¸»

ğŸ”¹ Donc : qui consulte quoi ?
â€¢	Le runtime (runc) : lit et Ã©crit dans le layout (state.json, pid, fifosâ€¦).
â€¢	Le shim : ne manipule pas directement les fichiers, mais il sâ€™appuie sur les commandes CLI (state, kill, delete, â€¦) qui elles-mÃªmes utilisent ce layout.
â€¢	containerd : ne touche pas au layout dâ€™Ã©tat â†’ il demande tout au shim, qui sâ€™appuie sur le runtime.

â¸»

ğŸ‘‰ En rÃ©sumÃ© :
â€¢	Oui, le shim consulte indirectement le layout dâ€™Ã©tat via les commandes du runtime.
â€¢	Câ€™est le runtime (runc/crun/â€¦) qui dÃ©finit et maintient ce layout.
â€¢	Le shim, lui, se contente de relayer les appels (il ne devrait pas rÃ©implÃ©menter la logique dâ€™Ã©tat).

Veux-tu que je te montre Ã  quoi ressemble exactement un state.json de runc et quelles infos le shim en retire pour rÃ©pondre Ã  containerd ?

----


Exactement âœ…, tu as bien rÃ©sumÃ© :
â€¢	Le shim v2 ne connaÃ®t pas les dÃ©tails internes du runtime (layout dâ€™Ã©tat, fichiers temporaires, etc.).
â€¢	Sa seule dÃ©pendance, câ€™est la faÃ§on dont il dialogue avec le runtime :
â€¢	soit via une CLI (comme runc, oÃ¹ il appelle runc state, runc kill, etc.),
â€¢	soit via une API interne (socket RPC, gRPC, agent dans une VM comme Kata/gVisor),
â€¢	soit via une lib si on lâ€™embarque (plus rare, mais possible).

Donc, si ton runtime expose la mÃªme interface CLI que runc, tu peux rÃ©utiliser le shim existant (containerd-shim-runc-v2).
Sinon, tu fais ton shim spÃ©cifique, mais il reste fin : juste un traducteur entre lâ€™API Task de containerd et lâ€™API/CLI de ton runtime.

ğŸ‘‰ En rÃ©sumÃ© : le shim nâ€™est pas couplÃ© au layout dâ€™Ã©tat, seulement au contrat dâ€™interface de ton runtime.

Veux-tu que je te prÃ©pare une checklist des appels CLI/outputs que ton runtime doit Ã©muler pour Ãªtre 100% compatible avec containerd-shim-runc-v2 sans recoder de shim ?

-----

Bonne question ğŸ‘Œ

ğŸ“Œ containerd expose deux types dâ€™API principales :
1.	gRPC API (la principale)
â€¢	Containerd expose un daemon (containerd) qui ouvre une socket Unix (par dÃ©faut /run/containerd/containerd.sock).
â€¢	Les clients comme ctr, nerdctl, ou le plugin CRI de Kubernetes parlent Ã  containerd via gRPC sur cette socket.
â€¢	Lâ€™API gRPC est trÃ¨s large : gestion dâ€™images, snapshots, conteneurs, tÃ¢ches, Ã©vÃ©nements, etc.
â€¢	La spec des services est dans containerd/api (fichiers .proto).
2.	CRI plugin (gRPC aussi, pour Kubernetes)
â€¢	Kubernetes nâ€™appelle pas directement lâ€™API native de containerd.
â€¢	kubelet parle au plugin CRI de containerd via gRPC (toujours sur une socket Unix, ex /run/containerd/containerd.sock).
â€¢	Ce plugin traduit les appels CRI (CreateContainer, StartContainer, etc.) en appels containerd internes.

ğŸ“Œ Pas dâ€™API HTTP/REST
â€¢	Containerd nâ€™expose pas de REST HTTP (contrairement Ã  Docker Engine qui expose une API HTTP/REST).
â€¢	Tout passe par gRPC (souvent avec TTRPC cÃ´tÃ© interne â€” une version plus lÃ©gÃ¨re pour les shims et process internes).

â¸»

ğŸ‘‰ Donc :
â€¢	Externe (clients/CRI) â†’ gRPC sur Unix socket.
â€¢	Interne (containerd â†” shim) â†’ TTRPC (plus lÃ©ger que gRPC).

â¸»

Veux-tu que je te montre un exemple concret de commande grpcurl que tu peux lancer sur /run/containerd/containerd.sock pour explorer lâ€™API gRPC de containerd ?

-----

Exactement âœ…
On peut dire quâ€™il y a deux couches de RPC autour de containerd, mais elles ne sont pas identiques :

â¸»

ğŸ”¹ 1. containerd â†” clients (docker, nerdctl, kubelet/CRI, ctrâ€¦)
â€¢	Protocole : gRPC â€œclassiqueâ€ (Protobuf, HTTP/2, socket Unix).
â€¢	Câ€™est lâ€™API publique de containerd.
â€¢	Elle couvre : images, snapshots, namespaces, tÃ¢ches, Ã©vÃ©nements, etc.
â€¢	Exemple de service : containerd.services.containers.v1.Containers.
â€¢	Câ€™est via Ã§a que kubelet parle au plugin CRI de containerd, ou que ctr/nerdctl fonctionnent.

â¸»

ğŸ”¹ 2. containerd â†” shim v2 (runc, crun, kata, gVisorâ€¦)
â€¢	Protocole : TTRPC (Tiny Transport RPC), une version plus lÃ©gÃ¨re que gRPC (mÃªme IDL, mais optimisÃ©e).
â€¢	Câ€™est lâ€™API interne dÃ©finie dans api/runtime/task/v2/task.proto.
â€¢	LÃ  on a le fameux service Task avec Create, Start, Kill, Exec, State, etc.
â€¢	Chaque shim implÃ©mente ce service et expose une socket (/run/containerd/.../shim.sock).
â€¢	Containerd envoie ses requÃªtes au shim via TTRPC.

â¸»

ğŸ”¹ Pourquoi deux couches ?
â€¢	gRPC externe â†’ API riche, extensible, standard, pour outils & orchestrateurs.
â€¢	TTRPC interne â†’ beaucoup plus lÃ©ger (pas de HTTP/2, pas de TLS, moins de copies mÃ©moire), adaptÃ© au hot path des conteneurs et aux nombreux shims quâ€™il faut gÃ©rer.

â¸»

ğŸ‘‰ Donc :
â€¢	Oui, il y a bien deux niveaux :
â€¢	gRPC pour â€œmonterâ€ (clients â†’ containerd),
â€¢	TTRPC pour â€œdescendreâ€ (containerd â†’ shims).

â¸»

Veux-tu que je te dessine un schÃ©ma ASCII clair avec ces deux niveaux (clients â†’ gRPC â†’ containerd â†’ TTRPC â†’ shim â†’ runtime) ?

---

Bonne prÃ©cision ğŸ‘Œ

ğŸ‘‰ Entre containerd et les shims v2, ce nâ€™est pas gRPC mais TTRPC.

â¸»

ğŸ“Œ Pourquoi TTRPC et pas gRPC ?
â€¢	TTRPC (Tiny Transport RPC) est une implÃ©mentation minimaliste, faite par les mainteneurs de containerd.
â€¢	Elle garde les .proto (donc mÃªmes dÃ©finitions quâ€™en gRPC) mais supprime tout lâ€™empilement HTTP/2, TLS, etc.
â€¢	RÃ©sultat :
â€¢	beaucoup plus rapide (moins dâ€™allocations, moins de copies mÃ©moire),
â€¢	binaire plus petit,
â€¢	idÃ©al pour la comm locale via une socket Unix entre containerd et le shim.

â¸»

ğŸ“Œ Donc la stack est :
â€¢	Clients externes â†’ containerd : gRPC â€œnormalâ€ (sur /run/containerd/containerd.sock).
â€¢	containerd â†’ shim v2 : TTRPC (sur une socket dÃ©diÃ©e du shim, ex /run/containerd/io.containerd.runtime.v2.task/.../shim.sock).

â¸»

ğŸ‘‰ En rÃ©sumÃ© :
â€¢	gRPC = API publique de containerd (pour kubelet, ctr, nerdctl, etc.).
â€¢	TTRPC = API interne entre containerd et les shims.

â¸»

Veux-tu que je te montre un petit extrait dâ€™un .proto (ex. task.proto) et son Ã©quivalent TTRPC, pour voir comment Ã§a se traduit concrÃ¨tement ?

------

Bonne question ğŸ‘Œ

Le nom TTRPC veut dire Tiny Transport RPC.
Et justement, son transport est trÃ¨s simple :

ğŸ“Œ Transport TTRPC
â€¢	Socket Unix (le plus courant)
â†’ chaque shim v2 ouvre une socket locale (shim.sock), et containerd sâ€™y connecte.
â€¢	Ou fichier/pipe (possible mais rare)
â†’ techniquement, TTRPC fonctionne tant quâ€™on a un FD type stream.

En clair, TTRPC se limite Ã  un flux de bytes point-Ã -point (pas de HTTP/2, pas de framing complexe).

â¸»

ğŸ“Œ Comment Ã§a marche
â€¢	Les .proto dÃ©finissent les services (ex : Create, Start, State du service Task).
â€¢	Un gÃ©nÃ©rateur TTRPC produit du code Go ou Rust avec :
â€¢	un codec binaire trÃ¨s proche de Protobuf,
â€¢	un framing minimal : chaque message = header (taille, type, ID) + payload protobuf.
â€¢	Containerd ouvre la socket du shim et envoie directement les appels RPC encodÃ©s â†’ le shim rÃ©pond de la mÃªme maniÃ¨re.

â¸»

ğŸ“Œ DiffÃ©rence avec gRPC
â€¢	gRPC = Protobuf + HTTP/2 (streaming, multiplexage, TLS, metadata).
â€¢	TTRPC = Protobuf + framing minimal + transport raw (socket Unix).
â€¢	ğŸ‘‰ RÃ©sultat : beaucoup plus lÃ©ger et rapide, mais limitÃ© Ã  de la comm locale.

â¸»

ğŸ‘‰ Donc : le transport TTRPC, câ€™est une socket Unix locale (raw stream), avec un protocole maison simplifiÃ© basÃ© sur Protobuf.

Veux-tu que je te dÃ©crive le framing dâ€™un message TTRPC (header + payload) pour voir concrÃ¨tement ce qui circule entre containerd et un shim ?

----


Bonne question ğŸ‘Œ

En Go, il existe une implÃ©mentation officielle de TTRPC (par les mainteneurs de containerd) et quelques extensions.

â¸»

ğŸ“Œ Librairies Go pour TTRPC
1.	github.com/containerd/ttrpc
â€¢	ğŸ“¦ lib principale et officielle.
â€¢	Fournit le client/serveur TTRPC, gÃ©nÃ©ration de stubs, gestion du framing et de la socket.
â€¢	Câ€™est celle utilisÃ©e dans containerd et containerd-shim-*.
2.	github.com/containerd/ttrpc/proto
â€¢	Outils pour gÃ©nÃ©rer du code Go TTRPC Ã  partir de .proto.
â€¢	Sâ€™utilise avec protoc + plugin protoc-gen-ttrpc.
3.	github.com/containerd/ttrpc/encoding
â€¢	Support de lâ€™encodage Protobuf, hooks pour en dÃ©finir dâ€™autres si besoin.

â¸»

ğŸ“Œ Workflow typique en Go
1.	Tu Ã©cris ton fichier .proto (exemple simplifiÃ©) :

syntax = "proto3";

package example.v1;

service Echo {
rpc SayHello(HelloRequest) returns (HelloReply);
}

message HelloRequest {
string name = 1;
}

message HelloReply {
string message = 1;
}


	2.	Tu gÃ©nÃ¨res le code Go TTRPC :

protoc --go_out=. --ttrpc_out=. api.proto


	3.	Tu obtiens :
	â€¢	api.ttrpc.pb.go â†’ interfaces et stubs TTRPC.
	â€¢	api.pb.go â†’ structs Protobuf classiques.
	4.	Dans ton shim ou serveur Go, tu implÃ©mentes lâ€™interface gÃ©nÃ©rÃ©e et tu dÃ©marres un serveur TTRPC sur une socket Unix.

â¸»

ğŸ‘‰ Donc, pour faire du TTRPC en Go, tu utilises principalement :
â€¢	containerd/ttrpc (lib de runtime),
â€¢	protoc-gen-ttrpc (plugin de gÃ©nÃ©ration).

â¸»

Veux-tu que je te prÃ©pare un exemple Go minimal (serveur + client TTRPC qui parlent via une socket Unix) pour voir Ã§a en pratique ?